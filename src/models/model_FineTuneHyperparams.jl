# This file was generated by the Julia OpenAPI Code Generator
# Do not modify this file directly. Modify the OpenAPI specification instead.


@doc raw"""FineTune_hyperparams
The hyperparameters used for the fine-tuning job. See the [fine-tuning guide](/docs/guides/legacy-fine-tuning/hyperparameters) for more details.

    FineTuneHyperparams(;
        batch_size=nothing,
        classification_n_classes=nothing,
        classification_positive_class=nothing,
        compute_classification_metrics=nothing,
        learning_rate_multiplier=nothing,
        n_epochs=nothing,
        prompt_loss_weight=nothing,
    )

    - batch_size::Int64 : The batch size to use for training. The batch size is the number of training examples used to train a single forward and backward pass. 
    - classification_n_classes::Int64 : The number of classes to use for computing classification metrics. 
    - classification_positive_class::String : The positive class to use for computing classification metrics. 
    - compute_classification_metrics::Bool : The classification metrics to compute using the validation dataset at the end of every epoch. 
    - learning_rate_multiplier::Float64 : The learning rate multiplier to use for training. 
    - n_epochs::Int64 : The number of epochs to train the model for. An epoch refers to one full cycle through the training dataset. 
    - prompt_loss_weight::Float64 : The weight to use for loss on the prompt tokens. 
"""
Base.@kwdef mutable struct FineTuneHyperparams <: OpenAPI.APIModel
    batch_size::Union{Nothing, Int64} = nothing
    classification_n_classes::Union{Nothing, Int64} = nothing
    classification_positive_class::Union{Nothing, String} = nothing
    compute_classification_metrics::Union{Nothing, Bool} = nothing
    learning_rate_multiplier::Union{Nothing, Float64} = nothing
    n_epochs::Union{Nothing, Int64} = nothing
    prompt_loss_weight::Union{Nothing, Float64} = nothing

    function FineTuneHyperparams(batch_size, classification_n_classes, classification_positive_class, compute_classification_metrics, learning_rate_multiplier, n_epochs, prompt_loss_weight, )
        OpenAPI.validate_property(FineTuneHyperparams, Symbol("batch_size"), batch_size)
        OpenAPI.validate_property(FineTuneHyperparams, Symbol("classification_n_classes"), classification_n_classes)
        OpenAPI.validate_property(FineTuneHyperparams, Symbol("classification_positive_class"), classification_positive_class)
        OpenAPI.validate_property(FineTuneHyperparams, Symbol("compute_classification_metrics"), compute_classification_metrics)
        OpenAPI.validate_property(FineTuneHyperparams, Symbol("learning_rate_multiplier"), learning_rate_multiplier)
        OpenAPI.validate_property(FineTuneHyperparams, Symbol("n_epochs"), n_epochs)
        OpenAPI.validate_property(FineTuneHyperparams, Symbol("prompt_loss_weight"), prompt_loss_weight)
        return new(batch_size, classification_n_classes, classification_positive_class, compute_classification_metrics, learning_rate_multiplier, n_epochs, prompt_loss_weight, )
    end
end # type FineTuneHyperparams

const _property_types_FineTuneHyperparams = Dict{Symbol,String}(Symbol("batch_size")=>"Int64", Symbol("classification_n_classes")=>"Int64", Symbol("classification_positive_class")=>"String", Symbol("compute_classification_metrics")=>"Bool", Symbol("learning_rate_multiplier")=>"Float64", Symbol("n_epochs")=>"Int64", Symbol("prompt_loss_weight")=>"Float64", )
OpenAPI.property_type(::Type{ FineTuneHyperparams }, name::Symbol) = Union{Nothing,eval(Base.Meta.parse(_property_types_FineTuneHyperparams[name]))}

function check_required(o::FineTuneHyperparams)
    o.batch_size === nothing && (return false)
    o.learning_rate_multiplier === nothing && (return false)
    o.n_epochs === nothing && (return false)
    o.prompt_loss_weight === nothing && (return false)
    true
end

function OpenAPI.validate_property(::Type{ FineTuneHyperparams }, name::Symbol, val)
end
